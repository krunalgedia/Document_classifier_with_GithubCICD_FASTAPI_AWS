{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e100450e-6284-4b96-b040-8861d00b2c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\kbged\\\\Downloads\\\\mlprojects\\\\Document_classifier_with_GithubCICD_FASTAPI_AWS\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf1a5d63-2bcc-443e-9c3a-a7ad225ff474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\kbged\\\\Downloads\\\\mlprojects\\\\Document_classifier_with_GithubCICD_FASTAPI_AWS'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ede3e70b-bd27-4621-8dd1-3e455fa176b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTrainingValidationConfig:\n",
    "    root_dir: Path\n",
    "    STATUS_FILE: str\n",
    "    unzip_dir: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6579c00b-af91-4f88-99ee-df35ac88da63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docClassify.constants import *\n",
    "from docClassify.utils.common import read_yaml, create_directories, compute_metrics, scale_bounding_box, create_bounding_box\n",
    "from docClassify.utils.common import DocumentClassificationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9353548d-36a8-47a6-b7d4-c1c7686c700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_training_validation_config(self) -> DataTrainingValidationConfig:\n",
    "        config = self.config.data_training_validation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_training_validation_config = DataTrainingValidationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            STATUS_FILE=config.STATUS_FILE,\n",
    "            unzip_dir=config.unzip_dir\n",
    "        )\n",
    "\n",
    "        return data_training_validation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d1bd585-51eb-4e21-8f28-03df28a877cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from docClassify.logger import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6c03d3d-1f33-48f7-b611-74925a57e3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "from PIL import Image\n",
    "from transformers import LayoutLMv3FeatureExtractor, LayoutLMv3TokenizerFast, LayoutLMv3Processor, LayoutLMv3ForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class TrainAndValidate:\n",
    "    def __init__(self, config: DataTrainingValidationConfig):\n",
    "        self.feature_extractor = LayoutLMv3FeatureExtractor(apply_ocr=False)\n",
    "        self.tokenizer = LayoutLMv3TokenizerFast.from_pretrained(\"microsoft/layoutlmv3-base\")\n",
    "        self.processor = LayoutLMv3Processor(self.feature_extractor, self.tokenizer)\n",
    "        self.config = config\n",
    "        self.DOCUMENT_CLASSES = sorted(list(map(lambda p: p.name,Path(self.config.unzip_dir).glob(\"*\"))))    \n",
    "\n",
    "    def get_train_test_path(self):\n",
    "        # Convert PosixPath objects to strings\n",
    "        image_paths=sorted(list(Path(self.config.unzip_dir).glob(\"*/*.png\")))\n",
    "        image_paths_str = [str(path) for path in image_paths]\n",
    "        \n",
    "        # Define labels based on whether the paths contain specific strings\n",
    "        income_labels = [\"income\" in path for path in image_paths_str]\n",
    "        balance_labels = [\"balance\" in path for path in image_paths_str]\n",
    "        cashflow_labels = [\"cashflow\" in path for path in image_paths_str]\n",
    "        \n",
    "        # Use any one of the labels as the target for stratified split\n",
    "        # Here, I'm using income_labels, but you can choose based on your requirements\n",
    "        train_images_str, test_images_str = train_test_split(image_paths_str, test_size=0.2, stratify=income_labels, random_state=42)\n",
    "        \n",
    "        # Convert back to PosixPath objects\n",
    "        train_images = [Path(path) for path in train_images_str]\n",
    "        test_images = [Path(path) for path in test_images_str]\n",
    "\n",
    "        return train_images, test_images\n",
    "    \n",
    "    def train(self, train_images, test_images):\n",
    "        train_dataset = DocumentClassificationDataset(train_images, self.processor)\n",
    "        valid_dataset = DocumentClassificationDataset(test_images, self.processor)\n",
    "        \n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=1,\n",
    "            shuffle=True,\n",
    "            #num_workers=10\n",
    "        )\n",
    "        \n",
    "        valid_dataloader = DataLoader(\n",
    "            valid_dataset,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            #num_workers=10\n",
    "        )\n",
    "\n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        n_classes = len(self.DOCUMENT_CLASSES)\n",
    "        \n",
    "        model = LayoutLMv3ForSequenceClassification.from_pretrained(\n",
    "                    \"microsoft/layoutlmv3-base\",\n",
    "                    num_labels=n_classes\n",
    "                )\n",
    "        model.to(device)\n",
    "\n",
    "        # load seqeval metric\n",
    "        #metric = evaluate.load(\"seqeval\")\n",
    "        model.config.id2label = {k: v for k, v in enumerate(self.DOCUMENT_CLASSES)}\n",
    "        model.config.label2id = {v: k for k, v in enumerate(self.DOCUMENT_CLASSES)}\n",
    "        # labels of the model\n",
    "        ner_labels = list(model.config.id2label.values())\n",
    "        \n",
    "        num_epochs = 1\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)\n",
    "        \n",
    "        # Initialize an empty DataFrame to store the metrics\n",
    "        columns = [\"Epoch\", \"Training Loss\", \"Validation Loss\", \"Precision\", \"Recall\", \"F1\", \"Accuracy\"]\n",
    "        df_metrics = pd.DataFrame(columns=columns)\n",
    "        \n",
    "        # Early stopping parameters\n",
    "        patience = 3 # Number of epochs to wait for improvement\n",
    "        best_validation_loss = float('inf')\n",
    "        current_patience = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"Epoch:\", epoch)\n",
    "        \n",
    "            # Training\n",
    "            model.train()\n",
    "            training_loss = 0.0\n",
    "            num = 0\n",
    "            for batch in tqdm(train_dataloader):\n",
    "                labels = torch.Tensor(batch[\"labels\"]).unsqueeze_(0).long().to(device)\n",
    "                outputs = model(\n",
    "                    input_ids=batch[\"input_ids\"].to(device),\n",
    "                    attention_mask=torch.tensor(batch[\"attention_mask\"]).to(device),\n",
    "                    bbox=torch.tensor(batch[\"bbox\"]).to(device),\n",
    "                    pixel_values=torch.tensor(batch[\"pixel_values\"]).to(device),\n",
    "                    labels=batch[\"labels\"].to(device)\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "                training_loss += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                num += 1\n",
    "        \n",
    "            print(\"Training Loss:\", training_loss / num)\n",
    "        \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            preds = []\n",
    "            labs = []\n",
    "            validation_loss = 0.0\n",
    "            num = 0\n",
    "            for batch in tqdm(valid_dataloader):\n",
    "                labels = torch.Tensor(batch[\"labels\"]).to(device)\n",
    "                outputs = model(\n",
    "                    input_ids=batch[\"input_ids\"].to(device),\n",
    "                    attention_mask=torch.tensor(batch[\"attention_mask\"]).to(device),\n",
    "                    bbox=torch.tensor(batch[\"bbox\"]).to(device),\n",
    "                    pixel_values=torch.tensor(batch[\"pixel_values\"]).to(device),\n",
    "                    labels=labels\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "                preds_idx = outputs.logits.argmax(axis=1)\n",
    "                labs.append(labels.tolist())\n",
    "                preds.append(preds_idx.tolist())\n",
    "                validation_loss += loss.item()\n",
    "                num += 1\n",
    "        \n",
    "            print(\"Validation Loss:\", validation_loss / num)\n",
    "            print(preds)\n",
    "            print(labs)\n",
    "        \n",
    "            overall_precision, overall_recall, overall_f1, overall_accuracy = compute_metrics([preds, labs])\n",
    "            print(\"Overall Precision:\", overall_precision)\n",
    "            print(\"Overall Recall:\", overall_recall)\n",
    "        \n",
    "            # Store metrics in the DataFrame\n",
    "            metrics_data = {\n",
    "                \"Epoch\": epoch,\n",
    "                \"Training Loss\": training_loss,\n",
    "                \"Validation Loss\": validation_loss,\n",
    "                \"Precision\": overall_precision,\n",
    "                \"Recall\": overall_recall,\n",
    "                \"F1\": overall_f1,\n",
    "                \"Accuracy\": overall_accuracy\n",
    "            }\n",
    "            #df_metrics = df_metrics.append(metrics_data, ignore_index=True)\n",
    "            df_metrics.loc[len(df_metrics)] = metrics_data\n",
    "        \n",
    "            # Early stopping check\n",
    "            if validation_loss < best_validation_loss:\n",
    "                best_validation_loss = validation_loss\n",
    "                current_patience = 0\n",
    "            else:\n",
    "                current_patience += 1\n",
    "                if current_patience >= patience:\n",
    "                    print(f\"Early stopping! No improvement in validation loss for {patience} consecutive epochs.\")\n",
    "                    break\n",
    "        \n",
    "        # Save the DataFrame to a CSV file or do any further analysis\n",
    "        df_metrics.to_csv(\"metrics.csv\", index=False)\n",
    "        print(df_metrics)\n",
    "\n",
    "        return df_metrics\n",
    "        \n",
    "        # Convert DataFrame to markdown\n",
    "        #markdown_table = df_metrics.to_markdown()\n",
    "        \n",
    "        # Print the markdown table\n",
    "        #print(markdown_table)\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "109bb448-6518-4e99-8711-ddde8af686be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-06 12:03:27,659: INFO: common: yaml file: src\\docClassify\\config\\config.yaml loaded successfully]\n",
      "[2024-02-06 12:03:27,665: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-02-06 12:03:27,668: INFO: common: created directory at: artifacts]\n",
      "[2024-02-06 12:03:27,672: INFO: common: created directory at: artifacts/data_training_validation]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kbged\\Miniconda3\\envs\\doc_classify_aws\\Lib\\site-packages\\transformers\\models\\layoutlmv3\\feature_extraction_layoutlmv3.py:30: FutureWarning: The class LayoutLMv3FeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use LayoutLMv3ImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Some weights of LayoutLMv3ForSequenceClassification were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b78ff9d999294a72bdb1a14a702397a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kbged\\AppData\\Local\\Temp\\ipykernel_3828\\2638552435.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask=torch.tensor(batch[\"attention_mask\"]).to(device),\n",
      "C:\\Users\\kbged\\AppData\\Local\\Temp\\ipykernel_3828\\2638552435.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bbox=torch.tensor(batch[\"bbox\"]).to(device),\n",
      "C:\\Users\\kbged\\AppData\\Local\\Temp\\ipykernel_3828\\2638552435.py:100: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pixel_values=torch.tensor(batch[\"pixel_values\"]).to(device),\n",
      "C:\\Users\\kbged\\Miniconda3\\envs\\doc_classify_aws\\Lib\\site-packages\\transformers\\modeling_utils.py:907: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.1226830914616586\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223dd139f1274a8cb2e0b4aa25553dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kbged\\AppData\\Local\\Temp\\ipykernel_3828\\2638552435.py:122: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask=torch.tensor(batch[\"attention_mask\"]).to(device),\n",
      "C:\\Users\\kbged\\AppData\\Local\\Temp\\ipykernel_3828\\2638552435.py:123: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bbox=torch.tensor(batch[\"bbox\"]).to(device),\n",
      "C:\\Users\\kbged\\AppData\\Local\\Temp\\ipykernel_3828\\2638552435.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pixel_values=torch.tensor(batch[\"pixel_values\"]).to(device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.0879846274852754\n",
      "[[0], [0], [2], [2], [2], [2], [2], [0], [2], [2]]\n",
      "[[0], [1], [2], [0], [0], [2], [2], [1], [1], [0]]\n",
      "Overall Precision: 0.26190476190476186\n",
      "Overall Recall: 0.4\n",
      "   Epoch  Training Loss  Validation Loss  Precision  Recall        F1  \\\n",
      "0      0      44.907324        10.879846   0.261905     0.4  0.294286   \n",
      "\n",
      "   Accuracy  \n",
      "0       0.4  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kbged\\Miniconda3\\envs\\doc_classify_aws\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_training_validation_config = config.get_data_training_validation_config()\n",
    "    #data_training_validation = DataTrainingValidationConfig(config=data_training_validation_config)\n",
    "    train_and_validate = TrainAndValidate(data_training_validation_config)\n",
    "    train_images, test_images = train_and_validate.get_train_test_path()\n",
    "    df = train_and_validate.train(train_images, test_images)\n",
    "    #data_training_validation.prepare_all_files()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b67406fa-ff7d-42c2-8ad6-1abc5f53dc91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['balance sheet', 'cashflow', 'income statement']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images\n",
    "sorted(list(map(lambda p: p.name,test_images[0].parent.parent.glob(\"*\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4d507a-07e5-4b10-a664-5973472485f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28b17c8-8c54-4537-bb5d-680829d5d289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c290c555-c495-42be-bf74-153788ced063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eace4a3-ffac-4e44-9a75-c1c742bf5380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d8e3761-c0ac-47d5-8939-6f176e3e8bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kbged\\Miniconda3\\envs\\doc_classify_aws\\Lib\\site-packages\\transformers\\models\\layoutlmv3\\feature_extraction_layoutlmv3.py:30: FutureWarning: The class LayoutLMv3FeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use LayoutLMv3ImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb9709890634b9285b014fcb1cd37b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kbged\\Miniconda3\\envs\\doc_classify_aws\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kbged\\.cache\\huggingface\\hub\\models--microsoft--layoutlmv3-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edfb9dcd799a40bdbd53fbf0cf72bb52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "341111dfdd3e4b94a0f67f98fe5bd864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.14k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_extractor = LayoutLMv3FeatureExtractor(apply_ocr=False)\n",
    "tokenizer = LayoutLMv3TokenizerFast.from_pretrained(\"microsoft/layoutlmv3-base\")\n",
    "processor = LayoutLMv3Processor(feature_extractor, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fdd948d-83a8-4574-aa3d-b13ddbdcd847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('artifacts/data_ingestion/data/balance sheet/bs7.png'),\n",
       " WindowsPath('artifacts/data_ingestion/data/cashflow/cf9.png'),\n",
       " WindowsPath('artifacts/data_ingestion/data/income statement/is4.png'),\n",
       " WindowsPath('artifacts/data_ingestion/data/balance sheet/bs16.png'),\n",
       " WindowsPath('artifacts/data_ingestion/data/balance sheet/bs15.png'),\n",
       " WindowsPath('artifacts/data_ingestion/data/income statement/is8.png'),\n",
       " WindowsPath('artifacts/data_ingestion/data/income statement/is2.png'),\n",
       " WindowsPath('artifacts/data_ingestion/data/cashflow/cf5.png'),\n",
       " WindowsPath('artifacts/data_ingestion/data/cashflow/cf12.png'),\n",
       " WindowsPath('artifacts/data_ingestion/data/balance sheet/bs3.png')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from pathlib import Path\n",
    "\n",
    "# Convert PosixPath objects to strings\n",
    "image_paths=sorted(list(Path(\"artifacts/data_ingestion/data/\").glob(\"*/*.png\")))\n",
    "image_paths_str = [str(path) for path in image_paths]\n",
    "\n",
    "# Define labels based on whether the paths contain specific strings\n",
    "income_labels = [\"income\" in path for path in image_paths_str]\n",
    "balance_labels = [\"balance\" in path for path in image_paths_str]\n",
    "cashflow_labels = [\"cashflow\" in path for path in image_paths_str]\n",
    "\n",
    "# Use any one of the labels as the target for stratified split\n",
    "# Here, I'm using income_labels, but you can choose based on your requirements\n",
    "train_images_str, test_images_str = train_test_split(image_paths_str, test_size=0.2, stratify=income_labels, random_state=42)\n",
    "\n",
    "# Convert back to PosixPath objects\n",
    "train_images = [Path(path) for path in train_images_str]\n",
    "test_images = [Path(path) for path in test_images_str]\n",
    "\n",
    "DOCUMENT_CLASSES = sorted(list(map(\n",
    "    lambda p: p.name,\n",
    "    Path(\"artifacts/data_ingestion/data/\").glob(\"*\")\n",
    ")))\n",
    "DOCUMENT_CLASSES\n",
    "test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "170f8ed1-1c4e-4130-8e80-e853cf119961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01f1ab35-ebc7-481d-93a7-a2ad9a25dea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DocumentClassificationDataset(train_images, processor)\n",
    "valid_dataset = DocumentClassificationDataset(test_images, processor)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    #num_workers=10\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    #num_workers=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77fbfe3d-841b-451a-9df3-00624f2bc313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feeb207367cb414f9ca209514a38fe6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForSequenceClassification were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LayoutLMv3ForSequenceClassification(\n",
       "  (layoutlmv3): LayoutLMv3Model(\n",
       "    (embeddings): LayoutLMv3TextEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (x_position_embeddings): Embedding(1024, 128)\n",
       "      (y_position_embeddings): Embedding(1024, 128)\n",
       "      (h_position_embeddings): Embedding(1024, 128)\n",
       "      (w_position_embeddings): Embedding(1024, 128)\n",
       "    )\n",
       "    (patch_embed): LayoutLMv3PatchEmbeddings(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (encoder): LayoutLMv3Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_pos_bias): Linear(in_features=32, out_features=12, bias=False)\n",
       "      (rel_pos_x_bias): Linear(in_features=64, out_features=12, bias=False)\n",
       "      (rel_pos_y_bias): Linear(in_features=64, out_features=12, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (classifier): LayoutLMv3ClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LayoutLMv3FeatureExtractor, LayoutLMv3TokenizerFast, LayoutLMv3Processor, LayoutLMv3ForSequenceClassification\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "n_classes = len(DOCUMENT_CLASSES)\n",
    "\n",
    "model = LayoutLMv3ForSequenceClassification.from_pretrained(\n",
    "            \"microsoft/layoutlmv3-base\",\n",
    "            num_labels=n_classes\n",
    "        )\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "166ff97e-4033-43e8-9513-7b54452b4ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2bb9ed9f-6cc3-46b0-bc47-83d7420e2d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1ba75c0ee514ebfad751a917c3134fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kbged\\AppData\\Local\\Temp\\ipykernel_51808\\124497835.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask=torch.tensor(batch[\"attention_mask\"]).to(device),\n",
      "C:\\Users\\kbged\\AppData\\Local\\Temp\\ipykernel_51808\\124497835.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bbox=torch.tensor(batch[\"bbox\"]).to(device),\n",
      "C:\\Users\\kbged\\AppData\\Local\\Temp\\ipykernel_51808\\124497835.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pixel_values=torch.tensor(batch[\"pixel_values\"]).to(device),\n",
      "C:\\Users\\kbged\\Miniconda3\\envs\\doc_classify_aws\\Lib\\site-packages\\transformers\\modeling_utils.py:907: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.086370076239109\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f3bf1ae55042958284e29579dbd28b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kbged\\AppData\\Local\\Temp\\ipykernel_51808\\124497835.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask=torch.tensor(batch[\"attention_mask\"]).to(device),\n",
      "C:\\Users\\kbged\\AppData\\Local\\Temp\\ipykernel_51808\\124497835.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bbox=torch.tensor(batch[\"bbox\"]).to(device),\n",
      "C:\\Users\\kbged\\AppData\\Local\\Temp\\ipykernel_51808\\124497835.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pixel_values=torch.tensor(batch[\"pixel_values\"]).to(device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.083499014377594\n",
      "[[0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
      "[[0], [1], [2], [0], [0], [2], [2], [1], [1], [0]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'compute_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 79\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(preds)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(labs)\n\u001b[1;32m---> 79\u001b[0m overall_precision, overall_recall, overall_f1, overall_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_metrics\u001b[49m([preds, labs])\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverall Precision:\u001b[39m\u001b[38;5;124m\"\u001b[39m, overall_precision)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverall Recall:\u001b[39m\u001b[38;5;124m\"\u001b[39m, overall_recall)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'compute_metrics' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# load seqeval metric\n",
    "#metric = evaluate.load(\"seqeval\")\n",
    "model.config.id2label = {k: v for k, v in enumerate(DOCUMENT_CLASSES)}\n",
    "model.config.label2id = {v: k for k, v in enumerate(DOCUMENT_CLASSES)}\n",
    "# labels of the model\n",
    "ner_labels = list(model.config.id2label.values())\n",
    "\n",
    "num_epochs = 1\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)\n",
    "\n",
    "# Initialize an empty DataFrame to store the metrics\n",
    "columns = [\"Epoch\", \"Training Loss\", \"Validation Loss\", \"Precision\", \"Recall\", \"F1\", \"Accuracy\"]\n",
    "df_metrics = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 3 # Number of epochs to wait for improvement\n",
    "best_validation_loss = float('inf')\n",
    "current_patience = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch:\", epoch)\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    training_loss = 0.0\n",
    "    num = 0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        labels = torch.Tensor(batch[\"labels\"]).unsqueeze_(0).long().to(device)\n",
    "        outputs = model(\n",
    "            input_ids=batch[\"input_ids\"].to(device),\n",
    "            attention_mask=torch.tensor(batch[\"attention_mask\"]).to(device),\n",
    "            bbox=torch.tensor(batch[\"bbox\"]).to(device),\n",
    "            pixel_values=torch.tensor(batch[\"pixel_values\"]).to(device),\n",
    "            labels=batch[\"labels\"].to(device)\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        training_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        num += 1\n",
    "\n",
    "    print(\"Training Loss:\", training_loss / num)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labs = []\n",
    "    validation_loss = 0.0\n",
    "    num = 0\n",
    "    for batch in tqdm(valid_dataloader):\n",
    "        labels = torch.Tensor(batch[\"labels\"]).to(device)\n",
    "        outputs = model(\n",
    "            input_ids=batch[\"input_ids\"].to(device),\n",
    "            attention_mask=torch.tensor(batch[\"attention_mask\"]).to(device),\n",
    "            bbox=torch.tensor(batch[\"bbox\"]).to(device),\n",
    "            pixel_values=torch.tensor(batch[\"pixel_values\"]).to(device),\n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        preds_idx = outputs.logits.argmax(axis=1)\n",
    "        labs.append(labels.tolist())\n",
    "        preds.append(preds_idx.tolist())\n",
    "        validation_loss += loss.item()\n",
    "        num += 1\n",
    "\n",
    "    print(\"Validation Loss:\", validation_loss / num)\n",
    "    print(preds)\n",
    "    print(labs)\n",
    "\n",
    "    overall_precision, overall_recall, overall_f1, overall_accuracy = compute_metrics([preds, labs])\n",
    "    print(\"Overall Precision:\", overall_precision)\n",
    "    print(\"Overall Recall:\", overall_recall)\n",
    "\n",
    "    # Store metrics in the DataFrame\n",
    "    metrics_data = {\n",
    "        \"Epoch\": epoch,\n",
    "        \"Training Loss\": training_loss,\n",
    "        \"Validation Loss\": validation_loss,\n",
    "        \"Precision\": overall_precision,\n",
    "        \"Recall\": overall_recall,\n",
    "        \"F1\": overall_f1,\n",
    "        \"Accuracy\": overall_accuracy\n",
    "    }\n",
    "    df_metrics = df_metrics.append(metrics_data, ignore_index=True)\n",
    "\n",
    "    # Early stopping check\n",
    "    if validation_loss < best_validation_loss:\n",
    "        best_validation_loss = validation_loss\n",
    "        current_patience = 0\n",
    "    else:\n",
    "        current_patience += 1\n",
    "        if current_patience >= patience:\n",
    "            print(f\"Early stopping! No improvement in validation loss for {patience} consecutive epochs.\")\n",
    "            break\n",
    "\n",
    "# Save the DataFrame to a CSV file or do any further analysis\n",
    "df_metrics.to_csv(\"metrics.csv\", index=False)\n",
    "df_metrics\n",
    "\n",
    "# Convert DataFrame to markdown\n",
    "markdown_table = df_metrics.to_markdown()\n",
    "\n",
    "# Print the markdown table\n",
    "print(markdown_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1504905b-46be-423e-989e-daed9d01ca93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4119af-8d60-4d64-98b1-c6cc211b34e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fcd00f-9513-41f9-9c56-37ff9ef127ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
