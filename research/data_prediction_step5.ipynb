{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d06a770-c208-4f1e-99be-878247056c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\kbged\\\\Downloads\\\\mlprojects\\\\Document_classifier_with_GithubCICD_FASTAPI_AWS'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2200c45f-529b-4610-8cee-6845e0537807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\kbged\\\\Downloads\\\\mlprojects\\\\Document_classifier_with_GithubCICD_FASTAPI_AWS\\\\research'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf64d8e1-dc1f-458e-bb62-0cdef53ba174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\kbged\\\\Downloads\\\\mlprojects\\\\Document_classifier_with_GithubCICD_FASTAPI_AWS'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d887dc8-84d1-43ba-b321-e5a92e7f31f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import LayoutLMv3FeatureExtractor, LayoutLMv3TokenizerFast, LayoutLMv3Processor, LayoutLMv3ForSequenceClassification\n",
    "import easyocr\n",
    "from docClassify.utils.common import read_yaml, create_directories, compute_metrics, scale_bounding_box, create_bounding_box\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b005bf7f-9075-42a6-b1e0-3f427960bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    def __init__(self, image_path):\n",
    "        self.model = LayoutLMv3ForSequenceClassification.from_pretrained(\"KgModel/IncomeStatement_Cashflow_BalanceStatement__Classifier_LayoutLMv3\")\n",
    "        self.feature_extractor = LayoutLMv3FeatureExtractor(apply_ocr=False)\n",
    "        self.tokenizer = LayoutLMv3TokenizerFast.from_pretrained(\"microsoft/layoutlmv3-base\")\n",
    "        self.processor = LayoutLMv3Processor(self.feature_extractor, self.tokenizer)\n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.reader = easyocr.Reader(['en'])\n",
    "        self.image_path = image_path\n",
    "\n",
    "    def predict(self):\n",
    "    \n",
    "        ocr_result = self.reader.readtext(str(image_path))\n",
    "        ocr_page = []\n",
    "        for bbox, word, confidence in ocr_result:\n",
    "            ocr_page.append({\n",
    "                \"word\": word, \"bounding_box\": create_bounding_box(bbox)\n",
    "            })\n",
    "\n",
    "        #with Image.open(image_path).convert(\"RGB\") as image:\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        width, height = image.size\n",
    "        width_scale = 1000 / width\n",
    "        height_scale = 1000 / height\n",
    "        words = []\n",
    "        boxes = []\n",
    "        for row in ocr_page:\n",
    "            boxes.append(scale_bounding_box(row[\"bounding_box\"], width_scale, height_scale))\n",
    "            words.append(row[\"word\"])\n",
    "\n",
    "        encoding = self.processor(\n",
    "            image,\n",
    "            words,\n",
    "            boxes=boxes,\n",
    "            max_length=512,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        outputs = self.model(\n",
    "                    input_ids=encoding[\"input_ids\"].to(self.device),\n",
    "                    attention_mask=torch.tensor(encoding[\"attention_mask\"]).to(self.device),\n",
    "                    bbox=torch.tensor(encoding[\"bbox\"]).to(self.device),\n",
    "                    pixel_values=torch.tensor(encoding[\"pixel_values\"]).to(self.device),\n",
    "                )\n",
    "        \n",
    "        preds_idx = outputs.logits.argmax(axis=1)\n",
    "        \n",
    "        return self.model.config.id2label[int(preds_idx[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2804d02-35c9-4d2d-92d7-e3913a72eae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-07 00:17:28,597: WARNING: easyocr: Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kbged\\AppData\\Local\\Temp\\ipykernel_22560\\2153649251.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask=torch.tensor(encoding[\"attention_mask\"]).to(self.device),\n",
      "C:\\Users\\kbged\\AppData\\Local\\Temp\\ipykernel_22560\\2153649251.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bbox=torch.tensor(encoding[\"bbox\"]).to(self.device),\n",
      "C:\\Users\\kbged\\AppData\\Local\\Temp\\ipykernel_22560\\2153649251.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pixel_values=torch.tensor(encoding[\"pixel_values\"]).to(self.device),\n"
     ]
    }
   ],
   "source": [
    "image_path = r\"C:\\Users\\kbged\\Downloads\\mlprojects\\Document_classifier_with_GithubCICD_FASTAPI_AWS\\artifacts\\data_ingestion\\data\\balance sheet\\bs1.png\"\n",
    "try:\n",
    "    predictor = Predictor(image_path)\n",
    "    prediction = predictor.predict() \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5be5a446-26c1-4901-a82e-cf718f6aa1a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'balance sheet'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7da052c8-7333-486c-a87c-058180f48ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_path = r\"C:\\Users\\kbged\\Downloads\\mlprojects\\Document_classifier_with_GithubCICD_FASTAPI_AWS\\artifacts\\data_ingestion\\data\\balance sheet\\bs1.png\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69d77080-c88e-448b-b4f2-90f987817aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2811c6c0-f020-4128-8897-ba320320d6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\kbged\\\\Downloads\\\\mlprojects\\\\Document_classifier_with_GithubCICD_FASTAPI_AWS\\\\research'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b878201-c0df-475a-ac88-de490b2dfba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da1c1fee-7b91-4ee9-954e-c0a80f53920e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69142d42-278c-42b7-acf0-b1b3f39be99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b88ca18-ac8f-4c04-919c-4095f3470424",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kbged\\Miniconda3\\envs\\doc_classify_aws\\Lib\\site-packages\\transformers\\models\\layoutlmv3\\feature_extraction_layoutlmv3.py:30: FutureWarning: The class LayoutLMv3FeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use LayoutLMv3ImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4273deb5-2a04-4b44-9cfd-efd5133ec69f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0, 12341,  2492,  6068,  8157,    12,  1092,    12,  2983,  8835,\n",
       "           12,  1092,    12,  2983, 20860, 18827, 28092,  1781,  7299, 44321,\n",
       "         4460,  1781,   290,     6,  1570,   316,     6, 34249,     4,   401,\n",
       "          158,     6, 39517,     4,   306, 20749,  4748,  4460,  1781,   379,\n",
       "          195,  6617,     4,   466,   195,  3367,     4,   406,  5143,    12,\n",
       "         1116,    12,  3698,  1781,   545, 30858,     4,   288, 21458,     4,\n",
       "          406,  1944,   251,    12,  1279,  5157,  4582,   601,     6,  1244,\n",
       "         7004,     4,   291,     4,   466,  1944,   251,    12,  1279, 22936,\n",
       "         1879,  6058,   601,     6,  1366,     6,  1244,  2491,     4,   306,\n",
       "         2929,     4,   406,  7858, 23474,   629,  1781, 18817,     4,   246,\n",
       "        11061,     4,   466,  5480,  4460,  1781,   508,     6, 39134,     4,\n",
       "          246,   365,     6,   398,  1922,     4,   246,  9149,  1781,    96,\n",
       "         9399,  9023,   753,   195,  4718,     4,   176, 40278,     4,   245,\n",
       "        19458, 22936,  1879,  6058,   195,     6,  1366,     6,  1244,   112,\n",
       "            6, 26373,     4,   398,   112,     6, 37767,     4,   398,  9149,\n",
       "          629, 22936,  1879,  6058,   365,   508,     4,   246,   973,     4,\n",
       "          245,  1944, 22936,  1879,  6058,   773, 12205,   564,   321,     4,\n",
       "          134,   112,     4,   288,  1944, 22936,  1879,  6058,   786,    12,\n",
       "        15979, 12205,   504,     6,  1244, 13955,     4,   288,  8176,     4,\n",
       "          246, 13915,  5526,  4068,     8, 31869,  1425,   291,     6,  1244,\n",
       "        30011,     4,   288,  3982,     4,   245,  7787,    12,  1279,  3227,\n",
       "          706,     6,  1244,  6164,     4,   306,  3330,     4,   398,  7871,\n",
       "            8,   827, 17172,   706,     6,  1244, 35058,     4,   466, 40567,\n",
       "            4,   246,  5480,   595,  1781,   132,     6, 37853,     4,   406,\n",
       "          132,     6, 32145,     4,   406, 36575, 17042, 18827,   545,     6,\n",
       "        20811,     4,   288,   501,     6,  3546,   245,     4,   288,  4584,\n",
       "         7982,   725, 20245,  4322,   108, 39135,  8662,  4248, 43913, 45490,\n",
       "         1702,  7509,   108,  2355,  1702,   812,   733,  5663,     4,   398,\n",
       "         5663,     4,   398,  1944,   812,  5694,   155,     6, 34890,     4,\n",
       "          306,   155,     6, 34890,     4,   306,  1223,  6486,  9762,  6114,\n",
       "          111,   306,     4,   398,   111,   306,     4,   398, 41737,  6114,\n",
       "        22300,     4,   398, 36205,     4,   466,  9944,  7153,  4709,  9523,\n",
       "          195,     6,   401,  3103,     4,   288,   204,     6,   466,  3079,\n",
       "            4,   398,  1702,  7509,   108,  2355, 18297,     7,  7783,   138,\n",
       "         4071,   361,     6, 35781,     4,   176,   290,     6,   406,  2881,\n",
       "            4,   134,  6965,    12, 10800, 20640,   773,  2631,     4,   306,\n",
       "         2107,     4,   401,  5480,  4071,   108,  2355,   361,     6, 40093,\n",
       "            4,   401,   290,     6,   406,  4027,     4,   406,  2597,    12,\n",
       "         1279, 13189, 13786, 12545,    13, 15131,   820,  4268,     4,   466,\n",
       "         3337,     4,   288,  1944,  7668,   883,   361,     4,   245,   508,\n",
       "            4,   466,  7858, 23474,   629, 13189,   365,   195,  6668,     4,\n",
       "          398,   204,  5479,     4,   288,  2597,    12,  1279, 13189,   773,\n",
       "        12205,   564,   155,     6, 40935,     4,   306,   132,     6, 26332,\n",
       "            4,   288,  1063,  3175, 13189,   545, 17445,     4,   245, 24621,\n",
       "            4,   466,  1944,   251,    12,  1279, 13189,   786,    12, 15979,\n",
       "        12205,   564, 18872,     4,   466, 15966,     4,   288,  5480,   251,\n",
       "           12,  1279, 13189,   155,     6,   466,  1898,     4,   288,   132,\n",
       "            6, 35795,     4,   398,  9149, 13189, 27860, 21467,   564, 36402,\n",
       "            4,   398, 35015,     4,   176, 17613,  3081,    31,   916,   564,\n",
       "        19446,     4,   246,  6705,     4,   466,  9149,   629, 13189,   365,\n",
       "         6791,     4,   134,  6121,     4,   466,  9149, 13189,   773, 12205,\n",
       "          564,     2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eaef0ac1-4aab-470d-ba6a-57b1eab82c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kbged\\AppData\\Local\\Temp\\ipykernel_16600\\2739523392.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask=torch.tensor(encoding[\"attention_mask\"]).to(device),\n",
      "C:\\Users\\kbged\\AppData\\Local\\Temp\\ipykernel_16600\\2739523392.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bbox=torch.tensor(encoding[\"bbox\"]).to(device),\n",
      "C:\\Users\\kbged\\AppData\\Local\\Temp\\ipykernel_16600\\2739523392.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pixel_values=torch.tensor(encoding[\"pixel_values\"]).to(device),\n",
      "C:\\Users\\kbged\\Miniconda3\\envs\\doc_classify_aws\\Lib\\site-packages\\transformers\\modeling_utils.py:907: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e456086-9308-4aee-96d4-957182b796c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'balance sheet'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10293099-2db0-44b9-ae03-ada4fd981da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(preds_idx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8659e23a-5ffd-4c83-916d-b96fae441897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "from PIL import Image\n",
    "from transformers import LayoutLMv3FeatureExtractor, LayoutLMv3TokenizerFast, LayoutLMv3Processor, LayoutLMv3ForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class Predict:\n",
    "    def __init__(self, config: DataPredictionConfig):\n",
    "        self.feature_extractor = LayoutLMv3FeatureExtractor(apply_ocr=False)\n",
    "        self.tokenizer = LayoutLMv3TokenizerFast.from_pretrained(\"microsoft/layoutlmv3-base\")\n",
    "        self.processor = LayoutLMv3Processor(self.feature_extractor, self.tokenizer)\n",
    "        self.config = config\n",
    "        self.DOCUMENT_CLASSES = sorted(list(map(lambda p: p.name,Path(self.config.unzip_dir).glob(\"*\"))))    \n",
    "\n",
    "    def get_train_test_path(self):\n",
    "        # Convert PosixPath objects to strings\n",
    "        image_paths=sorted(list(Path(self.config.unzip_dir).glob(\"*/*.png\")))\n",
    "        image_paths_str = [str(path) for path in image_paths]\n",
    "        \n",
    "        # Define labels based on whether the paths contain specific strings\n",
    "        income_labels = [\"income\" in path for path in image_paths_str]\n",
    "        balance_labels = [\"balance\" in path for path in image_paths_str]\n",
    "        cashflow_labels = [\"cashflow\" in path for path in image_paths_str]\n",
    "        \n",
    "        # Use any one of the labels as the target for stratified split\n",
    "        # Here, I'm using income_labels, but you can choose based on your requirements\n",
    "        train_images_str, test_images_str = train_test_split(image_paths_str, test_size=0.2, stratify=income_labels, random_state=42)\n",
    "        \n",
    "        # Convert back to PosixPath objects\n",
    "        train_images = [Path(path) for path in train_images_str]\n",
    "        test_images = [Path(path) for path in test_images_str]\n",
    "\n",
    "        return train_images, test_images\n",
    "    \n",
    "    def train(self, train_images, test_images):\n",
    "        train_dataset = DocumentClassificationDataset(train_images, self.processor)\n",
    "        valid_dataset = DocumentClassificationDataset(test_images, self.processor)\n",
    "        \n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=1,\n",
    "            shuffle=True,\n",
    "            #num_workers=10\n",
    "        )\n",
    "        \n",
    "        valid_dataloader = DataLoader(\n",
    "            valid_dataset,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            #num_workers=10\n",
    "        )\n",
    "\n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        n_classes = len(self.DOCUMENT_CLASSES)\n",
    "        \n",
    "        model = LayoutLMv3ForSequenceClassification.from_pretrained(\n",
    "                    \"microsoft/layoutlmv3-base\",\n",
    "                    num_labels=n_classes\n",
    "                )\n",
    "        model.to(device)\n",
    "\n",
    "        # load seqeval metric\n",
    "        #metric = evaluate.load(\"seqeval\")\n",
    "        model.config.id2label = {k: v for k, v in enumerate(self.DOCUMENT_CLASSES)}\n",
    "        model.config.label2id = {v: k for k, v in enumerate(self.DOCUMENT_CLASSES)}\n",
    "        # labels of the model\n",
    "        ner_labels = list(model.config.id2label.values())\n",
    "        \n",
    "        num_epochs = 1\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)\n",
    "        \n",
    "        # Initialize an empty DataFrame to store the metrics\n",
    "        columns = [\"Epoch\", \"Training Loss\", \"Validation Loss\", \"Precision\", \"Recall\", \"F1\", \"Accuracy\"]\n",
    "        df_metrics = pd.DataFrame(columns=columns)\n",
    "        \n",
    "        # Early stopping parameters\n",
    "        patience = 3 # Number of epochs to wait for improvement\n",
    "        best_validation_loss = float('inf')\n",
    "        current_patience = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"Epoch:\", epoch)\n",
    "        \n",
    "            # Training\n",
    "            model.train()\n",
    "            training_loss = 0.0\n",
    "            num = 0\n",
    "            for batch in tqdm(train_dataloader):\n",
    "                labels = torch.Tensor(batch[\"labels\"]).unsqueeze_(0).long().to(device)\n",
    "                outputs = model(\n",
    "                    input_ids=batch[\"input_ids\"].to(device),\n",
    "                    attention_mask=torch.tensor(batch[\"attention_mask\"]).to(device),\n",
    "                    bbox=torch.tensor(batch[\"bbox\"]).to(device),\n",
    "                    pixel_values=torch.tensor(batch[\"pixel_values\"]).to(device),\n",
    "                    labels=batch[\"labels\"].to(device)\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "                training_loss += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                num += 1\n",
    "        \n",
    "            print(\"Training Loss:\", training_loss / num)\n",
    "        \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            preds = []\n",
    "            labs = []\n",
    "            validation_loss = 0.0\n",
    "            num = 0\n",
    "            for batch in tqdm(valid_dataloader):\n",
    "                labels = torch.Tensor(batch[\"labels\"]).to(device)\n",
    "                outputs = model(\n",
    "                    input_ids=batch[\"input_ids\"].to(device),\n",
    "                    attention_mask=torch.tensor(batch[\"attention_mask\"]).to(device),\n",
    "                    bbox=torch.tensor(batch[\"bbox\"]).to(device),\n",
    "                    pixel_values=torch.tensor(batch[\"pixel_values\"]).to(device),\n",
    "                    labels=labels\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "                preds_idx = outputs.logits.argmax(axis=1)\n",
    "                labs.append(labels.tolist())\n",
    "                preds.append(preds_idx.tolist())\n",
    "                validation_loss += loss.item()\n",
    "                num += 1\n",
    "        \n",
    "            print(\"Validation Loss:\", validation_loss / num)\n",
    "            print(preds)\n",
    "            print(labs)\n",
    "        \n",
    "            overall_precision, overall_recall, overall_f1, overall_accuracy = compute_metrics([preds, labs])\n",
    "            print(\"Overall Precision:\", overall_precision)\n",
    "            print(\"Overall Recall:\", overall_recall)\n",
    "        \n",
    "            # Store metrics in the DataFrame\n",
    "            metrics_data = {\n",
    "                \"Epoch\": epoch,\n",
    "                \"Training Loss\": training_loss,\n",
    "                \"Validation Loss\": validation_loss,\n",
    "                \"Precision\": overall_precision,\n",
    "                \"Recall\": overall_recall,\n",
    "                \"F1\": overall_f1,\n",
    "                \"Accuracy\": overall_accuracy\n",
    "            }\n",
    "            #df_metrics = df_metrics.append(metrics_data, ignore_index=True)\n",
    "            df_metrics.loc[len(df_metrics)] = metrics_data\n",
    "        \n",
    "            # Early stopping check\n",
    "            if validation_loss < best_validation_loss:\n",
    "                best_validation_loss = validation_loss\n",
    "                current_patience = 0\n",
    "            else:\n",
    "                current_patience += 1\n",
    "                if current_patience >= patience:\n",
    "                    print(f\"Early stopping! No improvement in validation loss for {patience} consecutive epochs.\")\n",
    "                    break\n",
    "        \n",
    "        # Save the DataFrame to a CSV file or do any further analysis\n",
    "        df_metrics.to_csv(\"metrics.csv\", index=False)\n",
    "        print(df_metrics)\n",
    "\n",
    "        return df_metrics\n",
    "        \n",
    "        # Convert DataFrame to markdown\n",
    "        #markdown_table = df_metrics.to_markdown()\n",
    "        \n",
    "        # Print the markdown table\n",
    "        #print(markdown_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb3fc45-3628-47ab-a917-f0d7a138dd43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a5ae82-d7de-47bb-99d3-a597234dbfcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2da095a-523b-4a86-8d8e-9efe9a16e70c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf7147e-2c15-47e7-b2d2-16ffe349400c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e79679e-5fd7-486f-8eee-534bf46738b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
